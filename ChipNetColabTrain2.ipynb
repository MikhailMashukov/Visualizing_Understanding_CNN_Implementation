{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ChipNetColabTrain2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfPPQ6ztJhv4"
      },
      "source": [
        "Modified TorchVision 0.3 Object Detection finetuning tutorial (https://colab.research.google.com/github/pytorch/vision/blob/temp-tutorial/tutorials/torchvision_finetuning_instance_segmentation.ipynb), then my torchvision_finetuning_instance_segmentation.ipynb,\n",
        "then fixed usage of segmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBIoe_tHTQgV"
      },
      "source": [
        "%%shell\n",
        "\n",
        "cd /content\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "pip install cython\n",
        "# Install pycocotools, the version by default in Colab\n",
        "# has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n",
        "pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSdNNeknrAlH"
      },
      "source": [
        "!nvidia-smi\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd \"/content/gdrive/My Drive/INIRS\"\n",
        "!cp -r Images /content/\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t4TBwhHTdkd"
      },
      "source": [
        "import os\n",
        "\n",
        "# download the Penn-Fudan dataset\n",
        "if 0: # not os.path.exists('PennFudanPed'):\n",
        "    !wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip .\n",
        "    # extract it in the current folder\n",
        "    !unzip PennFudanPed.zip\n",
        "    !rm PennFudanPed/PNGImages/*[1-9].*\n",
        "    !rm PennFudanPed/PedMasks/*[1-9]_mask.*\n",
        "    # !rm PennFudanPed/PNGImages/*[4-9]0.*\n",
        "    # !rm PennFudanPed/PedMasks/*[4-9]0_mask.*\n",
        "    # !ls PennFudanPed/PNGImages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYDb7PBw55b-"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# # Download TorchVision repo to use some files from\n",
        "# # references/detection\n",
        "# git clone https://github.com/pytorch/vision.git\n",
        "pwd\n",
        "# cd vision\n",
        "# git checkout v0.3.0\n",
        "\n",
        "# cp references/segmentation/utils.py ../\n",
        "# cp references/segmentation/transforms.py ../\n",
        "# # cp references/segmentation/coco_eval.py ../\n",
        "# # cp references/segmentation/train.py ../     # Changed and loaded manually\n",
        "# cp references/segmentation/coco_utils.py ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyU6O-xUdjw5"
      },
      "source": [
        "# !cp /usr/local/lib/python3.6/dist-packages/torchvision/models/detection/mask_rcnn.py .\n",
        "# !pwd\n",
        "# !find / -name '*mask_rcnn*'\n",
        "# !pip freeze|grep torch\n",
        "%ls -l PyTorch/*trai*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTgWtixZTs3X"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "try:\n",
        "    from scipy.misc import imsave\n",
        "except:\n",
        "    from imageio import imsave \n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.nn.utils.prune as prune\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "# sys.path.append('PyTorch')\n",
        "from PyTorch.PyTChipNets import *\n",
        "from ChipJupyterNotUtils import *\n",
        "\n",
        "%matplotlib inline \n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "outDir = 'NetLogs'\n",
        "weightsFileNameTempl = outDir + '/ChipWeights_Epoch%d.h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UehCQ6LhTcE_"
      },
      "source": [
        "def printProgress(str):\n",
        "    with open(outDir + '/progress.log', 'a') as file:\n",
        "        file.write(str + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEARO4B_ye0s"
      },
      "source": [
        "# dataset = PennDataset('PennFudanPed/')\n",
        "# dataset = ChipDataset('/content')\n",
        "# dataset[0]\n",
        "# len(dataset)\n",
        "# for item in dataset:\n",
        "#     print(item[0].size)\n",
        "\n",
        "    # print(get_transform(False)(item[0]))\n",
        "#     print(item[1]['masks'].shape)\n",
        "# dataset[5][1]['masks'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l79ivkwKy357"
      },
      "source": [
        "from PyTorch import train\n",
        "from PyTorch.train import train_one_epoch, evaluate\n",
        "from PyTorch import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YFJGJxk6XEs"
      },
      "source": [
        "### Putting everything together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5dGaIezze3y"
      },
      "source": [
        "class DeepOptions:\n",
        "    if 0:\n",
        "        basePlaneCount=64\n",
        "        batchSize=4\n",
        "    else:\n",
        "        basePlaneCount=128\n",
        "        batchSize=1\n",
        "        \n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# device = torch.device('cpu')\n",
        "\n",
        "# dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
        "# dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
        "dataset = ChipDataset('/content/Images', get_transforms(train=True), DeepOptions.batchSize)\n",
        "dataset_test = ChipDataset('/content/Images', get_transforms(train=False))\n",
        "imageWeights = dataset.getWeights().to(device)\n",
        "\n",
        "# split the dataset in train and test set\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "if len(indices) > 50:\n",
        "    testImageCount = 50\n",
        "else:\n",
        "    testImageCount = len(indices) // 3\n",
        "\n",
        "if testImageCount > 0:\n",
        "    dataset = torch.utils.data.Subset(dataset, indices[:-testImageCount])\n",
        "    dataset_test = torch.utils.data.Subset(dataset_test, indices[-testImageCount:])\n",
        "else:\n",
        "    dataset = torch.utils.data.Subset(dataset, indices)\n",
        "    dataset_test = torch.utils.data.Subset(dataset_test, indices)\n",
        "train_sampler = torch.utils.data.RandomSampler(dataset)\n",
        "test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=16, num_workers=4,  # shuffle=True, \n",
        "    sampler=train_sampler, collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=1, num_workers=4,  # shuffle=False, \n",
        "    sampler=test_sampler, collate_fn=utils.collate_fn)\n",
        "\n",
        "# plt.imshow(imageWeights.numpy()) #[400:450, 500:700])\n",
        "# plt.colorbar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoenkCj18C4h"
      },
      "source": [
        "# import segmentation_models_pytorch \n",
        "\n",
        "# person or not # our dataset has two classes only - background and person\n",
        "num_classes = 1\n",
        "\n",
        "# get the model using our helper function\n",
        "# model = get_instance_segmentation_model(num_classes)\n",
        "model = ChipNet(num_classes, basePlaneCount=DeepOptions.basePlaneCount)\n",
        "# model = segmentation_models_pytorch .Unet(\"resnet18\", encoder_weights=\"imagenet\", classes=1, activation=None)\n",
        "# print(model)\n",
        "\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "epochNum = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF3xpsa6qN4K"
      },
      "source": [
        "!mkdir $outDir\n",
        "!mkdir $outDir/src\n",
        "!cp *.py $outDir/src\n",
        "!cp *.ipynb $outDir/src\n",
        "!cp -r PyTorch $outDir/src\n",
        "%pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGy009NTihZy"
      },
      "source": [
        "# pick one image from the test set\n",
        "# img, _ = dataset_test[0]\n",
        "img, target = dataset[0]\n",
        "# print(target['mask'].max())\n",
        "# plt.imshow(np.array(input.numpy().transpose(1, 2, 0), dtype=np.float32))\n",
        "plt.imshow(np.array(np.squeeze(target.numpy(), 0), dtype=np.float32))\n",
        "# img.to(device)\n",
        "img.shape, np.mean(img.numpy(), axis=(1, 2)), np.std(img.numpy(), axis=(1, 2))\n",
        "# target.shape, np.mean(target.numpy(), axis=(1, 2)), np.std(target.numpy(), axis=(1, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR890On5KiWs"
      },
      "source": [
        "def printWeightsStats(model):\n",
        "    for layerName in ['conv1', 'conv4']:  #  model.getAllLayers():\n",
        "        weights = model.getMultWeights(layerName)\n",
        "        infoStr = '%s: %s, %.5g, %.5g, %.5g, %.5g' % (layerName, str(weights.shape), \n",
        "                    weights.min(), weights.max(), np.mean(weights), np.std(weights))\n",
        "        print(infoStr)\n",
        "        printProgress(infoStr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQL5RnZ3Dy6E"
      },
      "source": [
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "# optimizer = torch.optim.SGD(params, lr=0.005,   # 0.005\n",
        "#                             momentum=0.9, weight_decay=0.0005, nesterov=True)\n",
        "optimizer = torch.optim.AdamW(params, lr=1.7e-4)\n",
        "criterion = train.getCriterion(imageWeights)\n",
        "\n",
        "# lr_scheduler = None\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=450,  # 3\n",
        "                                               gamma=0.5)\n",
        "# lr_scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, step_size_up=50,\n",
        "#                     base_lr=1e-4, max_lr=1e-3, cycle_momentum=True)\n",
        "\n",
        "epochNum = 3500  # 1270)\n",
        "model.loadState(weightsFileNameTempl % epochNum) \n",
        "# model.loadState('/content/ChipWeights_Epoch8000.h5')\n",
        "epochNum += 1\n",
        "\n",
        "# paramsToPrune = [(layer, 'weight') for name, layer in model.getAllLayers().items()]\n",
        "# prune.global_unstructured(paramsToPrune, pruning_method=prune.L1Unstructured,\n",
        "#             amount=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at-h4OWK0aoc"
      },
      "source": [
        "# let's train it for 10 epochs\n",
        "num_epochs = 5000\n",
        "\n",
        "i = 1\n",
        "t = 1\n",
        "if len(img.shape) == 3:\n",
        "    img.unsqueezdeve_(0)\n",
        "img = img.to(device)\n",
        "\n",
        "for _ in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        prediction = model(img)\n",
        "        prediction = prediction[0].cpu().numpy().transpose(1, 2, 0)\n",
        "        # prediction[prediction > 1] = 1\n",
        "        # prediction[prediction < 0] = 0\n",
        "        print(prediction.shape, prediction.dtype, prediction.min(), np.mean(prediction), prediction.max())\n",
        "\n",
        " # def train_one_epoch(model, criterion, optimizer, data_loader, lr_scheduler,\n",
        " # device, epochNum, print_freq):\n",
        "    \n",
        "    metric_logger = train_one_epoch(model, criterion, optimizer, data_loader, lr_scheduler, \\\n",
        "                    device, epochNum, print_freq=10)\n",
        "    if epochNum > 0 and epochNum % 10 == 0:\n",
        "        if epochNum > 0 and epochNum % 50 == 0:\n",
        "            model.saveState(weightsFileNameTempl % epochNum)\n",
        "\n",
        "        fig = plt.imshow(np.squeeze(prediction, 2),\n",
        "                vmin=-1, vmax=2, cmap='rainbow');\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "\n",
        "        prediction[prediction < 0] = 0\n",
        "        prediction[prediction > 2] = 2\n",
        "        if prediction.min() == prediction.max():\n",
        "            prediction[0, 0, 0] = 0\n",
        "        imsave(outDir + '/Pred_Epoch%d.png' % epochNum, prediction, format='png')\n",
        "        # printProgress('Epoch %d: learn. rate %.3g, loss %.7g' % \\\n",
        "        #                 (epochNum, optimizer.param_groups[0][\"lr\"])) \n",
        "\n",
        "        # fig, weightsImageData = showWeights(model.getMultWeights('conv1'), 16)\n",
        "        # plt.show()\n",
        "        # fig.savefig(outDir + '/Weights_conv1_Epoch%d.png' % epochNum, format='png', dpi=200)\n",
        "    printProgress('Epoch %d: %s\\n' % \\\n",
        "                    (epochNum, str(metric_logger)))\n",
        "    print('')\n",
        "    # printWeightsStats(model)\n",
        "    # update the learning rate\n",
        "    # lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    # evaluate(model, data_loader_test, device, num_classes)\n",
        "    # print(x)\n",
        "    epochNum += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHvwbq_8Icva"
      },
      "source": [
        " fig = plt.imshow(Image.fromarray(np.squeeze(prediction, 2)),\n",
        "               vmin=-0.01, vmax=0.01, cmap='Greys_r');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zsf6qOHkHZs_"
      },
      "source": [
        "print(train.i.shape, train.t.shape, train.i[0, :, :, 0])\n",
        "train.t.squeeze(1).shape\n",
        "nn.BCELoss(train.i, train.t)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDOk8mq2KIEN"
      },
      "source": [
        "input = torch.randn(3, 50, requires_grad=True)\n",
        "target = torch.randint(50, (3,), dtype=torch.int64)\n",
        "loss = nn.functional.cross_entropy(input, target)\n",
        "# input.shape\n",
        "# target\n",
        "loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iR6qrDuHtIB"
      },
      "source": [
        "nn.functional.cross_entropy(train.i, train.t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHwIdxH76uPj"
      },
      "source": [
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmN602iKsuey"
      },
      "source": [
        "Printing the prediction shows that we have a list of dictionaries. Each element of the list corresponds to a different image. As we have a single image, there is a single dictionary in the list.\n",
        "The dictionary contains the predictions for the image we passed. In this case, we can see that it contains `boxes`, `labels`, `masks` and `scores` as fields."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v5S3bm07SO1"
      },
      "source": [
        "Image.fromarray(prediction[0]['masks'][0, 0].mul(255).byte().cpu().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpqN9t1u7B2J"
      },
      "source": [
        "Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M58J3O9OtT1G"
      },
      "source": [
        "\n",
        "# Testing net performance/pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9i6geXwvUxf"
      },
      "source": [
        "parameters_to_prune = (\n",
        "    (model.conv1, 'weight'),\n",
        "    (model.conv4, 'weight'),\n",
        ")\n",
        "print(parameters_to_prune)\n",
        "\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    amount=0.2,\n",
        ")\n",
        "getSparsity(model.conv1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GP3WZmmIqZh-"
      },
      "source": [
        "metric_logger.loss.avg, metric_logger.loss.median\n",
        "# str(metric_logger.loss)\n",
        "\n",
        "def getSparsity(layer):\n",
        "    weights = layer.weight\n",
        "    return 100.0 * float(torch.sum(weights == 0)) / float(weights.nelement())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdqfFQ6MAwuZ"
      },
      "source": [
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn.functional as F\n",
        "\n",
        "epochNum = 3200\n",
        "# paramsToPrune = [(model.conv1, 'weight')] # (layer, 'weight') for name, layer in model.getAllLayers().items()]\n",
        "# print(paramsToPrune)\n",
        "\n",
        "img, target = dataset[0]\n",
        "img = img.to(device)\n",
        "if len(img.shape) == 3:\n",
        "    img.unsqueeze_(0)\n",
        "\n",
        "# model = ChipNet(num_classes, basePlaneCount=DeepOptions.basePlaneCount)\n",
        "# model.to(device)\n",
        "# model.loadState(weightsFileNameTempl % epochNum) \n",
        "prediction = model(img)\n",
        "loss = criterion(prediction, target.to(device))\n",
        "print('Initial loss %.6g' % (loss))\n",
        "\n",
        "for percent in range(5, 105, 10):\n",
        "    # model = ChipNet(num_classes, basePlaneCount=DeepOptions.basePlaneCount)\n",
        "    # model.to(device)\n",
        "    # model.loadState(weightsFileNameTempl % epochNum) \n",
        "    # model.loadState('/content/ChipWeights_Epoch8000.h5')\n",
        "    \n",
        "    paramsToPrune = [(layer, 'weight') for name, layer in model.getAllLayers().items()]\n",
        "    prune.global_unstructured(paramsToPrune, pruning_method=prune.L1Unstructured,\n",
        "            amount=percent / 100.0)\n",
        "    prediction = model(img)\n",
        "    loss = criterion(prediction, target.to(device))\n",
        "\n",
        "    sparsityInfoStr = ', '.join(['%.1f%%' % (getSparsity(layer)) \\\n",
        "                for name, layer in model.getAllLayers().items()])\n",
        "    print('Pruning %d%% - loss %.6g\\n  %s' % (percent, loss, sparsityInfoStr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ArDncr-j8YY"
      },
      "source": [
        "img = img.to(device)\n",
        "%timeit prediction = model(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfPcVJO8nFWs"
      },
      "source": [
        "model.eval()\n",
        "s = 0\n",
        "with torch.no_grad():\n",
        "    def test():\n",
        "        prediction = model(img)\n",
        "        global s\n",
        "        s += prediction[0].cpu().numpy().max()\n",
        "\n",
        "    %timeit test()\n",
        "    %timeit test()\n",
        "    %timeit test()\n",
        "    print(s)    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}