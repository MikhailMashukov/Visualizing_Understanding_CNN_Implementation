{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ChipNetColabTrain2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DfPPQ6ztJhv4"
      },
      "source": [
        "Modified TorchVision 0.3 Object Detection finetuning tutorial (https://colab.research.google.com/github/pytorch/vision/blob/temp-tutorial/tutorials/torchvision_finetuning_instance_segmentation.ipynb), then my torchvision_finetuning_instance_segmentation.ipynb,\n",
        "then fixed usage of segmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DBIoe_tHTQgV",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "pip install cython\n",
        "# Install pycocotools, the version by default in Colab\n",
        "# has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n",
        "pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QohknSE7xIk0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from pycocotools.coco import COCO\n",
        "# import requests\n",
        "\n",
        "# # !wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip .\n",
        "# !unzip annotations_trainval2017.zip\n",
        "# # instantiate COCO specifying the annotations json path\n",
        "# coco = COCO('annotations/instances_train2017.json')\n",
        "\n",
        "# cats = coco.loadCats(coco.getCatIds())\n",
        "# nms=[cat['name'] for cat in cats]\n",
        "# print('COCO categories: \\n{}\\n'.format(' '.join(nms)))\n",
        "\n",
        "# # get all images containing given categories (I'm selecting the \"bird\")\n",
        "# catIds = coco.getCatIds(catNms=['bird']);\n",
        "# imgIds = coco.getImgIds(catIds=catIds);\n",
        "\n",
        "# ds2 = train.get_dataset(\"coco\", \"train\", train.get_transform(True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_t4TBwhHTdkd",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "# download the Penn-Fudan dataset\n",
        "if not os.path.exists('PennFudanPed'):\n",
        "    !wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip .\n",
        "    # extract it in the current folder\n",
        "    !unzip PennFudanPed.zip\n",
        "    !rm PennFudanPed/PNGImages/*[1-9].*\n",
        "    !rm PennFudanPed/PedMasks/*[1-9]_mask.*\n",
        "    # !rm PennFudanPed/PNGImages/*[4-9]0.*\n",
        "    # !rm PennFudanPed/PedMasks/*[4-9]0_mask.*\n",
        "    # !ls PennFudanPed/PNGImages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UYDb7PBw55b-",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "\n",
        "# # Download TorchVision repo to use some files from\n",
        "# # references/detection\n",
        "# git clone https://github.com/pytorch/vision.git\n",
        "pwd\n",
        "cd vision\n",
        "# git checkout v0.3.0\n",
        "\n",
        "cp references/segmentation/utils.py ../\n",
        "cp references/segmentation/transforms.py ../\n",
        "# cp references/segmentation/coco_eval.py ../\n",
        "# cp references/segmentation/train.py ../     # Changed and loaded manually\n",
        "cp references/segmentation/coco_utils.py ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mTgWtixZTs3X",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "# import MyMaskRcnn\n",
        "from PyTChipNets import *\n",
        "\n",
        "%matplotlib inline \n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZEARO4B_ye0s",
        "colab": {}
      },
      "source": [
        "# dataset = PennDataset('PennFudanPed/')\n",
        "dataset = ChipDataset('/content')\n",
        "# dataset[0]\n",
        "for item in dataset:\n",
        "    print(item[0].size)\n",
        "\n",
        "    # print(get_transform(False)(item[0]))\n",
        "#     print(item[1]['masks'].shape)\n",
        "# dataset[5][1]['masks'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lWOhcsir9Ahx"
      },
      "source": [
        "So we can see that by default, the dataset returns a `PIL.Image` and a dictionary\n",
        "containing several fields, including `boxes`, `labels` and `masks`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyU6O-xUdjw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cp /usr/local/lib/python3.6/dist-packages/torchvision/models/detection/mask_rcnn.py .\n",
        "# !pwd\n",
        "# !find / -name '*mask_rcnn*'\n",
        "# !pip freeze|grep torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l79ivkwKy357",
        "colab": {}
      },
      "source": [
        "import train\n",
        "from train import train_one_epoch, criterion, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    targetSize = None   # (640, 512)\n",
        "    # targetSize = (32, 32)\n",
        "    transforms.append(Crop(250, 300, 704, 512))\n",
        "    if train:\n",
        "        # during training, randomly flip the training images\n",
        "        # and ground-truth for data augmentation\n",
        "        # transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "        transforms.append(torchvision.transforms.RandomHorizontalFlip())\n",
        "        transforms.append(torchvision.transforms.RandomVerticalFlip())\n",
        "    if not targetSize is None:\n",
        "        transforms.append(PadTo(targetSize, fill=0))\n",
        "        transforms.append(torchvision.transforms.CenterCrop((targetSize[1], targetSize[0])))\n",
        "    transforms.append(torchvision.transforms.ToTensor())\n",
        "    # return T.Compose(transforms)\n",
        "    return torchvision.transforms.Compose(transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3YFJGJxk6XEs"
      },
      "source": [
        "### Putting everything together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a5dGaIezze3y",
        "colab": {}
      },
      "source": [
        "# use our dataset and defined transformations\n",
        "# dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
        "# dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
        "dataset = ChipDataset('/content', get_transform(train=True))\n",
        "dataset_test = ChipDataset('/content', get_transform(train=False))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "if len(indices) > 50:\n",
        "    testImageCount = 50\n",
        "else:\n",
        "    testImageCount = len(indices) // 3\n",
        "\n",
        "if testImageCount > 0:\n",
        "    dataset = torch.utils.data.Subset(dataset, indices[:-testImageCount])\n",
        "    dataset_test = torch.utils.data.Subset(dataset_test, indices[-testImageCount:])\n",
        "else:\n",
        "    dataset = torch.utils.data.Subset(dataset, indices)\n",
        "    dataset_test = torch.utils.data.Subset(dataset_test, indices)\n",
        "train_sampler = torch.utils.data.RandomSampler(dataset)\n",
        "test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=16, num_workers=4,  # shuffle=True, \n",
        "    sampler=train_sampler, collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=1, num_workers=4,  # shuffle=False, \n",
        "    sampler=test_sampler, collate_fn=utils.collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zoenkCj18C4h",
        "colab": {}
      },
      "source": [
        "# import segmentation_models_pytorch \n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# device = torch.device('cpu')\n",
        "\n",
        "# person or not # our dataset has two classes only - background and person\n",
        "num_classes = 1\n",
        "\n",
        "# get the model using our helper function\n",
        "# model = get_instance_segmentation_model(num_classes)\n",
        "model = createSimpleChipNet(num_classes)\n",
        "# model = segmentation_models_pytorch .Unet(\"resnet18\", encoder_weights=\"imagenet\", classes=1, activation=None)\n",
        "# print(model)\n",
        "\n",
        "# move model to the right device\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQL5RnZ3Dy6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005,   # 0.005\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate by 10x every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15,  # 3\n",
        "                                               gamma=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGy009NTihZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pick one image from the test set\n",
        "# img, _ = dataset_test[0]\n",
        "img, target = dataset[0]\n",
        "# print(target['mask'].max())\n",
        "# plt.imshow(np.array(input.numpy().transpose(1, 2, 0), dtype=np.float32))\n",
        "plt.imshow(np.array(np.squeeze(target.numpy(), 0), dtype=np.float32))\n",
        "# img.to(device)\n",
        "img.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "at-h4OWK0aoc",
        "colab": {}
      },
      "source": [
        "# let's train it for 10 epochs\n",
        "num_epochs = 500\n",
        "\n",
        "i = 1\n",
        "t = 1\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        if len(img.shape) == 3:\n",
        "            img.unsqueeze_(0)\n",
        "        prediction = model(img.to(device))\n",
        "        prediction = prediction[0].cpu().numpy().transpose(1, 2, 0)\n",
        "        # prediction[prediction > 1] = 1\n",
        "        # prediction[prediction < 0] = 0\n",
        "        print(prediction.shape, prediction.dtype, prediction.min(), np.mean(prediction), prediction.max())\n",
        "    fig = plt.imshow(np.squeeze(prediction, 2),\n",
        "               vmin=-1, vmax=2, cmap='rainbow');\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        " # def train_one_epoch(model, criterion, optimizer, data_loader, lr_scheduler,\n",
        " # device, epoch, print_freq):\n",
        "    train_one_epoch(model, criterion, optimizer, data_loader, lr_scheduler, \\\n",
        "                    device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    # lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    # evaluate(model, data_loader_test, device, num_classes)\n",
        "    # print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHvwbq_8Icva",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " fig = plt.imshow(Image.fromarray(np.squeeze(prediction, 2)),\n",
        "               vmin=-0.01, vmax=0.01, cmap='Greys_r');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zsf6qOHkHZs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train.i.shape, train.t.shape, train.i[0, :, :, 0])\n",
        "train.t.squeeze(1).shape\n",
        "nn.BCELoss(train.i, train.t)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDOk8mq2KIEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input = torch.randn(3, 50, requires_grad=True)\n",
        "target = torch.randint(50, (3,), dtype=torch.int64)\n",
        "loss = nn.functional.cross_entropy(input, target)\n",
        "# input.shape\n",
        "# target\n",
        "loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iR6qrDuHtIB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn.functional.cross_entropy(train.i, train.t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YHwIdxH76uPj",
        "colab": {}
      },
      "source": [
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DmN602iKsuey"
      },
      "source": [
        "Printing the prediction shows that we have a list of dictionaries. Each element of the list corresponds to a different image. As we have a single image, there is a single dictionary in the list.\n",
        "The dictionary contains the predictions for the image we passed. In this case, we can see that it contains `boxes`, `labels`, `masks` and `scores` as fields."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5v5S3bm07SO1",
        "colab": {}
      },
      "source": [
        "Image.fromarray(prediction[0]['masks'][0, 0].mul(255).byte().cpu().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bpqN9t1u7B2J",
        "colab": {}
      },
      "source": [
        "Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M58J3O9OtT1G"
      },
      "source": [
        "And let's now visualize the top predicted segmentation mask. The masks are predicted as `[N, 1, H, W]`, where `N` is the number of predictions, and are probability maps between 0-1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdqfFQ6MAwuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}