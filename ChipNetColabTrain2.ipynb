{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ChipNetColabTrain2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DfPPQ6ztJhv4"
      },
      "source": [
        "Modified TorchVision 0.3 Object Detection finetuning tutorial (https://colab.research.google.com/github/pytorch/vision/blob/temp-tutorial/tutorials/torchvision_finetuning_instance_segmentation.ipynb), then my torchvision_finetuning_instance_segmentation.ipynb,\n",
        "then fixed usage of segmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DBIoe_tHTQgV",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "\n",
        "cd /content\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "pip install cython\n",
        "# Install pycocotools, the version by default in Colab\n",
        "# has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n",
        "pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSdNNeknrAlH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/home/gdrive')\n",
        "\n",
        "%cd \"/home/gdrive/My Drive/INIRS\"\n",
        "!cp -r Images /content/\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_t4TBwhHTdkd",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "# download the Penn-Fudan dataset\n",
        "if 0: # not os.path.exists('PennFudanPed'):\n",
        "    !wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip .\n",
        "    # extract it in the current folder\n",
        "    !unzip PennFudanPed.zip\n",
        "    !rm PennFudanPed/PNGImages/*[1-9].*\n",
        "    !rm PennFudanPed/PedMasks/*[1-9]_mask.*\n",
        "    # !rm PennFudanPed/PNGImages/*[4-9]0.*\n",
        "    # !rm PennFudanPed/PedMasks/*[4-9]0_mask.*\n",
        "    # !ls PennFudanPed/PNGImages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UYDb7PBw55b-",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "\n",
        "# # Download TorchVision repo to use some files from\n",
        "# # references/detection\n",
        "# git clone https://github.com/pytorch/vision.git\n",
        "pwd\n",
        "# cd vision\n",
        "# git checkout v0.3.0\n",
        "\n",
        "# cp references/segmentation/utils.py ../\n",
        "# cp references/segmentation/transforms.py ../\n",
        "# # cp references/segmentation/coco_eval.py ../\n",
        "# # cp references/segmentation/train.py ../     # Changed and loaded manually\n",
        "# cp references/segmentation/coco_utils.py ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mTgWtixZTs3X",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "try:\n",
        "    from scipy.misc import imsave\n",
        "except:\n",
        "    from imageio import imsave \n",
        "import torch\n",
        "import torch.utils.data\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "# sys.path.append('PyTorch')\n",
        "from PyTorch.PyTChipNets import *\n",
        "from ChipJupyterNotUtils import *\n",
        "\n",
        "%matplotlib inline \n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "outDir = 'NetLogs'\n",
        "weightsFileNameTempl = outDir + '/ChipWeights_Epoch%d.h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UehCQ6LhTcE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def printProgress(str):\n",
        "    with open(outDir + '/progress.log', 'a') as file:\n",
        "        file.write(str + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZEARO4B_ye0s",
        "colab": {}
      },
      "source": [
        "# dataset = PennDataset('PennFudanPed/')\n",
        "# dataset = ChipDataset('/content')\n",
        "# dataset[0]\n",
        "# len(dataset)\n",
        "# for item in dataset:\n",
        "#     print(item[0].size)\n",
        "\n",
        "    # print(get_transform(False)(item[0]))\n",
        "#     print(item[1]['masks'].shape)\n",
        "# dataset[5][1]['masks'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lWOhcsir9Ahx"
      },
      "source": [
        "So we can see that by default, the dataset returns a `PIL.Image` and a dictionary\n",
        "containing several fields, including `boxes`, `labels` and `masks`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyU6O-xUdjw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cp /usr/local/lib/python3.6/dist-packages/torchvision/models/detection/mask_rcnn.py .\n",
        "# !pwd\n",
        "# !find / -name '*mask_rcnn*'\n",
        "# !pip freeze|grep torch\n",
        "%ls -l PyTorch/*Chip*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l79ivkwKy357",
        "colab": {}
      },
      "source": [
        "from PyTorch import train\n",
        "from PyTorch.train import train_one_epoch, criterion, evaluate\n",
        "from PyTorch import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3YFJGJxk6XEs"
      },
      "source": [
        "### Putting everything together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a5dGaIezze3y",
        "colab": {}
      },
      "source": [
        "class DeepOptions:\n",
        "    if 0:\n",
        "        basePlaneCount=64\n",
        "        batchSize=4\n",
        "    else:\n",
        "        basePlaneCount=16\n",
        "        batchSize=1\n",
        "        \n",
        "# dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
        "# dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
        "dataset = ChipDataset('/content/Images', get_transforms(train=True), DeepOptions.batchSize)\n",
        "dataset_test = ChipDataset('/content/Images', get_transforms(train=False))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "if len(indices) > 50:\n",
        "    testImageCount = 50\n",
        "else:\n",
        "    testImageCount = len(indices) // 3\n",
        "\n",
        "if testImageCount > 0:\n",
        "    dataset = torch.utils.data.Subset(dataset, indices[:-testImageCount])\n",
        "    dataset_test = torch.utils.data.Subset(dataset_test, indices[-testImageCount:])\n",
        "else:\n",
        "    dataset = torch.utils.data.Subset(dataset, indices)\n",
        "    dataset_test = torch.utils.data.Subset(dataset_test, indices)\n",
        "train_sampler = torch.utils.data.RandomSampler(dataset)\n",
        "test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=16, num_workers=4,  # shuffle=True, \n",
        "    sampler=train_sampler, collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=1, num_workers=4,  # shuffle=False, \n",
        "    sampler=test_sampler, collate_fn=utils.collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zoenkCj18C4h",
        "colab": {}
      },
      "source": [
        "# import segmentation_models_pytorch \n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# device = torch.device('cpu')\n",
        "\n",
        "# person or not # our dataset has two classes only - background and person\n",
        "num_classes = 1\n",
        "\n",
        "# get the model using our helper function\n",
        "# model = get_instance_segmentation_model(num_classes)\n",
        "model = ChipNet(num_classes, basePlaneCount=DeepOptions.basePlaneCount)\n",
        "# model = segmentation_models_pytorch .Unet(\"resnet18\", encoder_weights=\"imagenet\", classes=1, activation=None)\n",
        "# print(model)\n",
        "\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "epochNum = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF3xpsa6qN4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir $outDir\n",
        "!mkdir $outDir/src\n",
        "!cp *.py $outDir/src\n",
        "!cp *.ipynb $outDir/src\n",
        "!cp -r PyTorch $outDir/src\n",
        "%pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGy009NTihZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pick one image from the test set\n",
        "# img, _ = dataset_test[0]\n",
        "img, target = dataset[0]\n",
        "# print(target['mask'].max())\n",
        "# plt.imshow(np.array(input.numpy().transpose(1, 2, 0), dtype=np.float32))\n",
        "plt.imshow(np.array(np.squeeze(target.numpy(), 0), dtype=np.float32))\n",
        "# img.to(device)\n",
        "img.shape, np.mean(img.numpy(), axis=(1, 2)), np.std(img.numpy(), axis=(1, 2))\n",
        "# target.shape, np.mean(target.numpy(), axis=(1, 2)), np.std(target.numpy(), axis=(1, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR890On5KiWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def printWeightsStats(model):\n",
        "    for layerName in model.getAllLayers():\n",
        "        weights = model.getMultWeights(layerName)\n",
        "        infoStr = '%s: %s, %.5g, %.5g, %.5g, %.5g' % (layerName, str(weights.shape), \n",
        "                    weights.min(), weights.max(), np.mean(weights), np.std(weights))\n",
        "        print(infoStr)\n",
        "        printProgress(infoStr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQL5RnZ3Dy6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005,   # 0.005\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate by 10x every 3 epochs\n",
        "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20,  # 3\n",
        "#                                                gamma=0.5)\n",
        "lr_scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, step_size_up=20,\n",
        "                                                 base_lr=0.001, max_lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "at-h4OWK0aoc",
        "colab": {}
      },
      "source": [
        "# let's train it for 10 epochs\n",
        "num_epochs = 500\n",
        "\n",
        "i = 1\n",
        "t = 1\n",
        "for _ in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        if len(img.shape) == 3:\n",
        "            img.unsqueeze_(0)\n",
        "        prediction = model(img.to(device))\n",
        "        prediction = prediction[0].cpu().numpy().transpose(1, 2, 0)\n",
        "        # prediction[prediction > 1] = 1\n",
        "        # prediction[prediction < 0] = 0\n",
        "        print(prediction.shape, prediction.dtype, prediction.min(), np.mean(prediction), prediction.max())\n",
        "    fig = plt.imshow(np.squeeze(prediction, 2),\n",
        "               vmin=-1, vmax=2, cmap='rainbow');\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        " # def train_one_epoch(model, criterion, optimizer, data_loader, lr_scheduler,\n",
        " # device, epochNum, print_freq):\n",
        "    \n",
        "    metric_logger = train_one_epoch(model, criterion, optimizer, data_loader, lr_scheduler, \\\n",
        "                    device, epochNum, print_freq=10)\n",
        "    if epochNum > 0 and epochNum % 10 == 0:\n",
        "        print(weightsFileNameTempl % epochNum)\n",
        "        model.saveState(weightsFileNameTempl % epochNum)\n",
        "        prediction[prediction < 0] = 0\n",
        "        prediction[prediction > 2] = 2\n",
        "        imsave(outDir + '/Pred_Epoch%d.png' % epochNum, prediction, format='png')\n",
        "        # printProgress('Epoch %d: learn. rate %.3g, loss %.7g' % \\\n",
        "        #                 (epochNum, optimizer.param_groups[0][\"lr\"])) \n",
        "        showWeights(model.getMultWeights('conv1'), 8)\n",
        "        plt.show()\n",
        "    printProgress('Epoch %d: %s' % \\\n",
        "                    (epochNum, str(metric_logger)))\n",
        "    printWeightsStats(model)\n",
        "    # update the learning rate\n",
        "    # lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    # evaluate(model, data_loader_test, device, num_classes)\n",
        "    # print(x)\n",
        "    epochNum += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHvwbq_8Icva",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " fig = plt.imshow(Image.fromarray(np.squeeze(prediction, 2)),\n",
        "               vmin=-0.01, vmax=0.01, cmap='Greys_r');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zsf6qOHkHZs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train.i.shape, train.t.shape, train.i[0, :, :, 0])\n",
        "train.t.squeeze(1).shape\n",
        "nn.BCELoss(train.i, train.t)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDOk8mq2KIEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input = torch.randn(3, 50, requires_grad=True)\n",
        "target = torch.randint(50, (3,), dtype=torch.int64)\n",
        "loss = nn.functional.cross_entropy(input, target)\n",
        "# input.shape\n",
        "# target\n",
        "loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iR6qrDuHtIB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn.functional.cross_entropy(train.i, train.t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YHwIdxH76uPj",
        "colab": {}
      },
      "source": [
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DmN602iKsuey"
      },
      "source": [
        "Printing the prediction shows that we have a list of dictionaries. Each element of the list corresponds to a different image. As we have a single image, there is a single dictionary in the list.\n",
        "The dictionary contains the predictions for the image we passed. In this case, we can see that it contains `boxes`, `labels`, `masks` and `scores` as fields."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5v5S3bm07SO1",
        "colab": {}
      },
      "source": [
        "Image.fromarray(prediction[0]['masks'][0, 0].mul(255).byte().cpu().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bpqN9t1u7B2J",
        "colab": {}
      },
      "source": [
        "Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M58J3O9OtT1G"
      },
      "source": [
        "And let's now visualize the top predicted segmentation mask. The masks are predicted as `[N, 1, H, W]`, where `N` is the number of predictions, and are probability maps between 0-1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdqfFQ6MAwuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}