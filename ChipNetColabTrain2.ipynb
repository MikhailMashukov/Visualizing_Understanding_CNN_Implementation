{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ChipNetColabTrain2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DfPPQ6ztJhv4"
      },
      "source": [
        "Modified TorchVision 0.3 Object Detection finetuning tutorial (https://colab.research.google.com/github/pytorch/vision/blob/temp-tutorial/tutorials/torchvision_finetuning_instance_segmentation.ipynb), then my torchvision_finetuning_instance_segmentation.ipynb,\n",
        "then fixed usage of segmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DBIoe_tHTQgV",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "pip install cython\n",
        "# Install pycocotools, the version by default in Colab\n",
        "# has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n",
        "pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QohknSE7xIk0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from pycocotools.coco import COCO\n",
        "# import requests\n",
        "\n",
        "# # !wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip .\n",
        "# !unzip annotations_trainval2017.zip\n",
        "# # instantiate COCO specifying the annotations json path\n",
        "# coco = COCO('annotations/instances_train2017.json')\n",
        "\n",
        "# cats = coco.loadCats(coco.getCatIds())\n",
        "# nms=[cat['name'] for cat in cats]\n",
        "# print('COCO categories: \\n{}\\n'.format(' '.join(nms)))\n",
        "\n",
        "# # get all images containing given categories (I'm selecting the \"bird\")\n",
        "# catIds = coco.getCatIds(catNms=['bird']);\n",
        "# imgIds = coco.getImgIds(catIds=catIds);\n",
        "\n",
        "# ds2 = train.get_dataset(\"coco\", \"train\", train.get_transform(True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_t4TBwhHTdkd",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "\n",
        "# download the Penn-Fudan dataset\n",
        "wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip .\n",
        "# extract it in the current folder\n",
        "unzip PennFudanPed.zip\n",
        "rm PennFudanPed/PNGImages/*[1-9].*\n",
        "rm PennFudanPed/PedMasks/*[1-9]_mask.*\n",
        "# rm PennFudanPed/PNGImages/*[4-9]0.*\n",
        "# rm PennFudanPed/PedMasks/*[4-9]0_mask.*\n",
        "# ls PennFudanPed/PNGImages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UYDb7PBw55b-",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "\n",
        "# # Download TorchVision repo to use some files from\n",
        "# # references/detection\n",
        "# git clone https://github.com/pytorch/vision.git\n",
        "pwd\n",
        "cd vision\n",
        "# git checkout v0.3.0\n",
        "\n",
        "cp references/segmentation/utils.py ../\n",
        "cp references/segmentation/transforms.py ../\n",
        "# cp references/segmentation/coco_eval.py ../\n",
        "# cp references/segmentation/train.py ../     # Changed and loaded manually\n",
        "cp references/segmentation/coco_utils.py ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mTgWtixZTs3X",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "# import MyMaskRcnn\n",
        "from PyTChipNets import *\n",
        "\n",
        "%matplotlib inline \n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "class PennFudanDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "        print('%d images, %d masks' % (len(self.imgs), len(self.masks)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images ad masks\n",
        "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        # note that we haven't converted the mask to RGB,\n",
        "        # because each color corresponds to a different instance\n",
        "        # with 0 being background\n",
        "        mask = Image.open(mask_path)\n",
        "\n",
        "        mask = np.array(mask)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = np.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LDjuVFgexFfh",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "# np.asarray(\n",
        "img = Image.open('PennFudanPed/PNGImages/FudanPed00020.png')\n",
        "print(img.size, torchvision.transforms.ToTensor()(img).shape)\n",
        "img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cFHKCvCTxiff",
        "colab": {}
      },
      "source": [
        "mask = Image.open('PennFudanPed/PedMasks/FudanPed00010_mask.png')\n",
        "# each mask instance has a different color, from zero to N, where\n",
        "# N is the number of instances. In order to make visualization easier,\n",
        "# let's adda color palette to the mask.\n",
        "mask.putpalette([\n",
        "    0, 0, 0, # black background\n",
        "    255, 0, 0, # index 1 is red\n",
        "    255, 255, 0, # index 2 is yellow\n",
        "    255, 153, 0, # index 3 is orange\n",
        "])\n",
        "mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZEARO4B_ye0s",
        "colab": {}
      },
      "source": [
        "dataset = ChipDataset('PennFudanPed/')\n",
        "# dataset[0]\n",
        "for item in dataset:\n",
        "    print(item[0].size)\n",
        "\n",
        "    # print(get_transform(False)(item[0]))\n",
        "#     print(item[1]['masks'].shape)\n",
        "# dataset[5][1]['masks'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lWOhcsir9Ahx"
      },
      "source": [
        "So we can see that by default, the dataset returns a `PIL.Image` and a dictionary\n",
        "containing several fields, including `boxes`, `labels` and `masks`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyU6O-xUdjw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cp /usr/local/lib/python3.6/dist-packages/torchvision/models/detection/mask_rcnn.py .\n",
        "# !pwd\n",
        "# !find / -name '*mask_rcnn*'\n",
        "# !pip freeze|grep torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-WXLwePV5ieP"
      },
      "source": [
        "\n",
        "## Training and evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YjNHjVMOyYlH",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "def get_instance_segmentation_model(num_classes):\n",
        "    # load an instance segmentation model pre-trained on COCO\n",
        "    model = MyMaskRcnn.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    # get the number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # now get the number of input features for the mask classifier\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    # and replace the mask predictor with a new one\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
        "                                                       hidden_layer,\n",
        "                                                       num_classes)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2u9e_pdv54nG"
      },
      "source": [
        "\n",
        "\n",
        "Let's write some helper functions for data augmentation / transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l79ivkwKy357",
        "colab": {}
      },
      "source": [
        "import train\n",
        "from train import train_one_epoch, criterion, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    targetSize = (640, 512)\n",
        "    # targetSize = (32, 32)\n",
        "    # if train:\n",
        "    #     # during training, randomly flip the training images\n",
        "    #     # and ground-truth for data augmentation\n",
        "    #     # transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    #     transforms.append(torchvision.transforms.RandomHorizontalFlip(0.5))\n",
        "    transforms.append(PadTo(targetSize, fill=0))\n",
        "    transforms.append(torchvision.transforms.CenterCrop((targetSize[1], targetSize[0])))\n",
        "    transforms.append(torchvision.transforms.ToTensor())\n",
        "    # return T.Compose(transforms)\n",
        "    return torchvision.transforms.Compose(transforms)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3YFJGJxk6XEs"
      },
      "source": [
        "### Putting everything together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a5dGaIezze3y",
        "colab": {}
      },
      "source": [
        "# use our dataset and defined transformations\n",
        "# dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
        "# dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
        "dataset = ChipDataset('PennFudanPed', get_transform(train=True))\n",
        "dataset_test = ChipDataset('PennFudanPed', get_transform(train=False))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "if len(indices) > 50:\n",
        "    testImageCount = 50\n",
        "else:\n",
        "    testImageCount = len(indices) // 3\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:-testImageCount])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[-testImageCount:])\n",
        "train_sampler = torch.utils.data.RandomSampler(dataset)\n",
        "test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=16, num_workers=4,  # shuffle=True, \n",
        "    sampler=train_sampler, collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=1, num_workers=4,  # shuffle=False, \n",
        "    sampler=test_sampler, collate_fn=utils.collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zoenkCj18C4h",
        "colab": {}
      },
      "source": [
        "# import segmentation_models_pytorch \n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# device = torch.device('cpu')\n",
        "\n",
        "# person or not # our dataset has two classes only - background and person\n",
        "num_classes = 1\n",
        "\n",
        "# get the model using our helper function\n",
        "# model = get_instance_segmentation_model(num_classes)\n",
        "model = createSimpleChipNet(num_classes)\n",
        "# model = segmentation_models_pytorch .Unet(\"resnet18\", encoder_weights=\"imagenet\", classes=1, activation=None)\n",
        "# print(model)\n",
        "\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.001,   # 0.005\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate by\n",
        "# 10x every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=30,  # 3\n",
        "                                               gamma=0.999)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGy009NTihZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pick one image from the test set\n",
        "# img, _ = dataset_test[0]\n",
        "img, target = dataset[0]\n",
        "# print(target['mask'].max())\n",
        "# plt.imshow(np.array(input.numpy().transpose(1, 2, 0), dtype=np.float32))\n",
        "plt.imshow(np.array(np.squeeze(target.numpy(), 0), dtype=np.float32))\n",
        "# img.to(device)\n",
        "img.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "at-h4OWK0aoc",
        "colab": {}
      },
      "source": [
        "# let's train it for 10 epochs\n",
        "num_epochs = 500\n",
        "\n",
        "i = 1\n",
        "t = 1\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        if len(img.shape) == 3:\n",
        "            img.unsqueeze_(0)\n",
        "        prediction = model(img.to(device))\n",
        "        prediction = prediction[0].cpu().numpy().transpose(1, 2, 0)\n",
        "        # prediction[prediction > 1] = 1\n",
        "        # prediction[prediction < 0] = 0\n",
        "        print(prediction.shape, prediction.dtype, prediction.min(), np.mean(prediction), prediction.max())\n",
        "    fig = plt.imshow(np.squeeze(prediction, 2),\n",
        "               vmin=-1, vmax=2, cmap='rainbow');\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        " # def train_one_epoch(model, criterion, optimizer, data_loader, lr_scheduler,\n",
        " # device, epoch, print_freq):\n",
        "    train_one_epoch(model, criterion, optimizer, data_loader, lr_scheduler, \\\n",
        "                    device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    # lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_test, device, num_classes)\n",
        "    # print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHvwbq_8Icva",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " fig = plt.imshow(Image.fromarray(np.squeeze(prediction, 2)),\n",
        "               vmin=-0.01, vmax=0.01, cmap='Greys_r');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zsf6qOHkHZs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train.i.shape, train.t.shape, train.i[0, :, :, 0])\n",
        "train.t.squeeze(1).shape\n",
        "nn.BCELoss(train.i, train.t)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDOk8mq2KIEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input = torch.randn(3, 50, requires_grad=True)\n",
        "target = torch.randint(50, (3,), dtype=torch.int64)\n",
        "loss = nn.functional.cross_entropy(input, target)\n",
        "# input.shape\n",
        "# target\n",
        "loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iR6qrDuHtIB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn.functional.cross_entropy(train.i, train.t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YHwIdxH76uPj",
        "colab": {}
      },
      "source": [
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DmN602iKsuey"
      },
      "source": [
        "Printing the prediction shows that we have a list of dictionaries. Each element of the list corresponds to a different image. As we have a single image, there is a single dictionary in the list.\n",
        "The dictionary contains the predictions for the image we passed. In this case, we can see that it contains `boxes`, `labels`, `masks` and `scores` as fields."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5v5S3bm07SO1",
        "colab": {}
      },
      "source": [
        "Image.fromarray(prediction[0]['masks'][0, 0].mul(255).byte().cpu().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bpqN9t1u7B2J",
        "colab": {}
      },
      "source": [
        "Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M58J3O9OtT1G"
      },
      "source": [
        "And let's now visualize the top predicted segmentation mask. The masks are predicted as `[N, 1, H, W]`, where `N` is the number of predictions, and are probability maps between 0-1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdqfFQ6MAwuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}