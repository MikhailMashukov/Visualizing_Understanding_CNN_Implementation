{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training AlexNet with tips and checks on how to train CNNs: Practical CNNs in PyTorch(1)\n",
    "\n",
    "https://medium.com/@kushajreal/training-alexnet-with-tips-and-checks-on-how-to-train-cnns-practical-cnns-in-pytorch-1-61daa679c74a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesFolder = '/root/Visualiz_Zeiler/ImageNet/'\n",
    "\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%mkdir -p QtLogs/src\n",
    "%cp -f * QtLogs/src/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 Create Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My Data dirs explained. For my train dataset I use the validation dataset provided by Imagenet i.e. 50000 images. All these images are stored under the train folder. One preprocessing step that I done is to rescale all these images, so that their shorter side is 256. To do this you can either add a transform.Resize(256) or from the terminal run this command and all their images would be rescaled to 256 and stored on disk\n",
    "```(python)\n",
    "    find . -name \"*.JPEG\" | xargs -I {} convert {} -resize \"256^>\" {}\n",
    "```\n",
    "\n",
    "For my validation data, I use 10 images from each class from the train dataset of Imagenet. Below I give the script I used to do so. These images are also rescaled to 256 on the shorter side using the above command. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Run this cell to get 10 images from train folder and place them in your val folder\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "source_dir = '../../../Data/ILSVRC/Data/CLS-LOC/train/'\n",
    "dest_dir = '../../../Data/ILSVRC2012/val/'\n",
    "dirs = os.listdir(source_dir)\n",
    "dirs.sort()\n",
    "\n",
    "for dir in dirs:\n",
    "    os.makedirs(dest_dir + dir)\n",
    "    path_source = source_dir + dir\n",
    "    path_dest = dest_dir + dir\n",
    "    images = os.listdir(path_source)\n",
    "    \n",
    "    for i in range(10):\n",
    "        shutil.copy(path_source + '/' + images[i], path_dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create the dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the transforms we first need to crop our images, as the input size for our model is 224. For the training dataset I use RandomHorizontalFlip as a data augmentation technique. Another technique that is useful is the FiveCrop transform. But I did not use that here. The images are normalized using the standard values of mean and std computed over the entire ImageNet.\n",
    "\n",
    "For the validation I did not use data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = imagesFolder + '/train'\n",
    "val_dir = imagesFolder + '/test'\n",
    "\n",
    "size = 224\n",
    "batch_size = 128\n",
    "num_workers = 20\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "}\n",
    "\n",
    "image_datasets = {\n",
    "    'train': ImageFolder(train_dir, transform=data_transforms['train']),\n",
    "    'val': ImageFolder(val_dir, transform=data_transforms['val']),\n",
    "}\n",
    "\n",
    "data_loader = {\n",
    "    x: torch.utils.data.DataLoader(image_datasets[x],\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=True,\n",
    "                                   num_workers=num_workers) for x in ['train', 'val']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Plot some images to test everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For imagenet we need to do some preprocessing for the labels\n",
    "# f = open(\"../../../Data/LOC_synset_mapping.txt\", \"r\")\n",
    "f = open(imagesFolder + '/ILSVRC2012_map_clsloc.txt', 'r')\n",
    "labels_dict = {}\n",
    "labels_list = []\n",
    "for line in f:\n",
    "    split = line.split(' ', maxsplit=1)\n",
    "    split[1] = split[1][:-1]\n",
    "    label_id, label = split[0], split[1]\n",
    "    labels_dict[label_id] = label\n",
    "    labels_list.append(split[1])\n",
    "   \n",
    "print('Labels dict:-')\n",
    "for idx, (key, value) in enumerate(labels_dict.items()):\n",
    "    print(key, value)\n",
    "    if (idx > 3):\n",
    "        break\n",
    "        \n",
    "print('\\nLabels list (just 0 indexed instead of file names)')\n",
    "labels_list[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our images are normalized so denormalize then and convert them to numpy\n",
    "def imshow(img, title=None):\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std*img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "if 0:\n",
    "    selectedImages = []\n",
    "    selectedLabels = []\n",
    "    it = iter(data_loader['train'])\n",
    "    while len(selectedImages) < 10:\n",
    "        images, labels = next(it)\n",
    "        for i, l in enumerate(labels):\n",
    "    #         print(l)\n",
    "            if l == 0:\n",
    "                selectedImages.append(images[i])\n",
    "                selectedLabels.append(l)\n",
    "    #             print(len(selectedImages))\n",
    "\n",
    "    images, labels = next(iter(data_loader['train']))\n",
    "    grid_img = make_grid(selectedImages[:10], nrow=5)\n",
    "    plt.figure(figsize=(15, 8), dpi=150)\n",
    "    imshow(grid_img)\n",
    "    print ([labels_list[x] for x in selectedLabels[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 Model Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Activation function:- ReLU is the default choice. But LeakyReLU is also good. Use LeakyReLU in GANs always.\n",
    "2. Weight Initialization:- Use He initialization as default with ReLU.Â \n",
    "3. Preprocess data:- There are two choices normalizing between [-1,1] or using (x-mean)/std approch. Your choice for this.\n",
    "4. Batch Normalization:- Apply before non-linearity i.e. ReLU. For the values of the mean and variance use the running average of the values while training as test time. PyTorch automatically maintains this for you. Note: In a recent review paper for ICLR 2019, FixUp initialization was introduced. Using it, you don't need batchnorm layers in your model.\n",
    "5. Pooling layers:- Apply after non-linearity i.e. ReLU. Different tasks would require different pooling methods for classification max-pool is default.\n",
    "6. Optimizer:- Adam is a good choice, SDG+momentum+nesterov is also good. fast.ai recently announced a new opitimizer AdamW. Choice of optiimzer comes to experimentation and the task at hand. Look for some benchmarks for different optimizers that can guide your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv_base = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2, bias=False),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.fc_base = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256*6*6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_base(x)\n",
    "        x = x.view(x.size(0), 256*6*6)\n",
    "        x = self.fc_base(x)\n",
    "        return x\n",
    "    \n",
    "model = AlexNet()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use indexing to index the layers, but in future cases we would name the layers\n",
    "conv_list = [0, 4, 8, 10, 12]\n",
    "fc_list = [1, 4, 6]\n",
    "for i in conv_list:\n",
    "    torch.nn.init.kaiming_normal_(model.conv_base[i].weight)\n",
    "for i in fc_list:\n",
    "    torch.nn.init.kaiming_normal_(model.fc_base[i].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create opimizer, learning_rate scheduler, loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We train everything on GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Cross entropy loss takes the logits directly, so we don't need to apply softmax in our CNN\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0005)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize weights\n",
    "conv_list = [0, 4, 8, 10, 12]\n",
    "fc_list = [1, 4, 6]\n",
    "for i in conv_list:\n",
    "    torch.nn.init.kaiming_normal_(model.conv_base[i].weight)\n",
    "for i in fc_list:\n",
    "    torch.nn.init.kaiming_normal_(model.fc_base[i].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printProgress(str):\n",
    "    with open('QtLogs/progress.log', 'a') as file:\n",
    "        file.write(str + '\\n')\n",
    "        \n",
    "# A simple train loop that you can use. You can seperate different train and val functions also.\n",
    "def train(model, data_loader, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    reportFreq = 100\n",
    "\n",
    "    since = time.time()\n",
    "    \n",
    "    train_batch_loss = []\n",
    "    train_epoch_loss = []\n",
    "    val_epoch_loss = []\n",
    "    blockNum = 0\n",
    "    valLossInfo = 'val. loss 0, val. acc 0'\n",
    "    for epoch in range(num_epochs):\n",
    "#         print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "#         print('-'*15)\n",
    "        \n",
    "        # You perform validation test after every epoch\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            blockLoss = 0\n",
    "            blockCorrects = 0\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            for idx, (inputs, labels) in enumerate(data_loader[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # zero accumulated gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # During train phase we want to remember history for grads\n",
    "                # and during val we do not want history of grads\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "#                     print(loss.item(), inputs.size(0))\n",
    "#                     print(preds)\n",
    "#                     print('labels.data ', labels.data)\n",
    "                    blockLoss += loss.item()\n",
    "                    blockCorrects += torch.sum(preds == labels.data)\n",
    "                    if idx % reportFreq == reportFreq - 1:\n",
    "                        train_batch_loss.append(loss.item())\n",
    "                        if phase == 'train':\n",
    "                            blockNum += 1\n",
    "                            printProgress('Epoch %d: loss %.7g, acc %.6f, %s ' \\\n",
    "                                          '(actual epoch: %d)' % \\\n",
    "                                  (blockNum,\n",
    "                                   blockLoss / reportFreq, \n",
    "                                   float(blockCorrects) / reportFreq / inputs.size(0),\n",
    "                                   valLossInfo, epoch + 1))\n",
    "                            print('Epoch %d: %d/%d steps, loss %.7g, acc %.5f' % \\\n",
    "                                  (epoch + 1, idx + 1, len(data_loader[phase]),\n",
    "                                   blockLoss / reportFreq, \n",
    "                                   float(blockCorrects) / reportFreq / inputs.size(0)))\n",
    "#                             valLossInfo = 'val. loss 0, val. acc 0'\n",
    "                        blockLoss = 0\n",
    "                        blockCorrects = 0\n",
    "#                         break\n",
    "                        \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            epoch_loss = running_loss / len(data_loader[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(data_loader[phase].dataset)\n",
    "            \n",
    "            print('Epoch {} {} Loss: {:.5f} Acc: {:.5f}'.format(\n",
    "                    epoch + 1, phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            if phase == 'val':\n",
    "                val_epoch_loss.append((epoch_loss, epoch_acc))\n",
    "                valLossInfo = 'val. loss %.7g, val. acc %.6f' % (epoch_loss, epoch_acc)\n",
    "                if scheduler:\n",
    "                    scheduler.step(loss.item())\n",
    "            else:\n",
    "                train_epoch_loss.append((epoch_loss, epoch_acc))\n",
    "                \n",
    "#         print()\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(list):\n",
    "    pass\n",
    "\n",
    "data = {'train': MyData(), 'val': MyData()}\n",
    "for phase in ['train', 'val']:\n",
    "    it = iter(data_loader[phase])\n",
    "    for _ in range(201):\n",
    "#         print(next(it))\n",
    "        data[phase].append(next(it))\n",
    "    data[phase].dataset = range(201 * batch_size)\n",
    "    assert batch_size == len(data[phase][0][0])\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5, weight_decay=0.0005)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True,\n",
    "#                                                  factor=0.2, threshold=1e-6, patience=30)\n",
    "scheduler = None\n",
    "train(model, data_loader, criterion, optimizer, scheduler, num_epochs=200)\n",
    "# train(model, data, criterion, optimizer, scheduler, num_epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.0005)\n",
    "train(model, data_loader, criterion, optimizer, scheduler, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=0.0005)\n",
    "train(model, data_loader, criterion, optimizer, scheduler, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-6, weight_decay=0.0005)\n",
    "train(model, data_loader, criterion, optimizer, scheduler, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-7, weight_decay=0.0005)\n",
    "train(model, data_loader, criterion, optimizer, scheduler, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
